{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "data = load_data('../data/salsa-non-adjudicated/salsa_train.json')\n",
    "systems, edit_types = set([x['system'] for x in data]), set(data[0]['annotations'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency & Composition of Structure Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtypes = [\n",
    "    'Voice Change',\n",
    "    'Part-of-Speech Change',\n",
    "    'Tense Change',\n",
    "    'Grammatical Number',\n",
    "    'Clausal Change'\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(7, 3))\n",
    "quality = np.array([35, 25, 25, 10, 5])\n",
    "error = np.array([25, 30, 25, 15, 5])\n",
    "\n",
    "color = color_mapping['structure']\n",
    "scalar = [0.8, 1, 1.2, 1.4, 1.6]\n",
    "colors = []\n",
    "for s in scalar:\n",
    "    colors += [colorscale(color, s)]\n",
    "\n",
    "axs[0].pie(quality, colors=colors, labels=subtypes, labeldistance=None)\n",
    "axs[1].pie(error, colors=colors, labels=subtypes, labeldistance=None)\n",
    "\n",
    "axs[0].set_xlabel('Quality Structure Changes')\n",
    "axs[1].set_xlabel('Error Structure Changes')\n",
    "axs[1].legend(subtypes, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.suptitle(\"Manual Breakdown of Structure Changes\")\n",
    "\n",
    "out_filename = \"../paper/plot/appendix/structure-breakdown.pdf\"\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "wedgeprops = {\n",
    "    \"edgecolor\" : \"black\",\n",
    "    'linewidth': 0.7,\n",
    "    'antialiased': True\n",
    "}\n",
    "\n",
    "labels, values = zip(*count_dataset_composite_edits(data, 'structure').items())\n",
    "axs[0].pie(values, colors=[color_mapping[l] for l in labels], labels=labels, labeldistance=1.2, \\\n",
    "    wedgeprops=wedgeprops, autopct='%1.f%%', pctdistance=0.7, textprops={'fontsize': 10})\n",
    "\n",
    "labels, values = zip(*count_dataset_composite_edits(data, 'split').items())\n",
    "axs[1].pie(values, colors=[color_mapping[l] for l in labels], labels=labels, labeldistance=1.2, \\\n",
    "    wedgeprops=wedgeprops, autopct='%1.f%%', pctdistance=0.7, textprops={'fontsize': 10})\n",
    "\n",
    "axs[0].set_xlabel('Structure Edits', fontsize=14)\n",
    "axs[1].set_xlabel('Split Edits', fontsize=14)\n",
    "# plt.suptitle(\"Child Edits of Composite Edits\")\n",
    "\n",
    "# axs[1].legend([x.capitalize() for x in labels], loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "#     handlelength=1,handleheight=1,facecolor='white')\n",
    "\n",
    "out_filename = \"../paper/plot/appendix/composite-breakdown.pdf\"\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate Among Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_human = [s for s in data if 'Human' not in s['system']]\n",
    "\n",
    "all_edits = [i for j in [s['processed_annotations'] for s in not_human] for i in j]\n",
    "error_edits = [e for e in all_edits if e['error_type'] != None]\n",
    "\n",
    "print(f'% all edits that are errors: {(len(error_edits)/len(all_edits)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [sum([e['error_type'] != None for e in s['processed_annotations']]) for s in data]\n",
    "\n",
    "human = [s for s in data if 'Human' in s['system']]\n",
    "gpt_few = [s for s in data if 'few' in s['system']]\n",
    "gpt_zero = [s for s in data if 'zero' in s['system']]\n",
    "muss = [s for s in data if 'Muss' in s['system']]\n",
    "\n",
    "# Percent of all sentences that contain an error\n",
    "def error_rate(data, exclude_bad_deletion=False):\n",
    "    count = 0\n",
    "    if exclude_bad_deletion:\n",
    "        for s in data:\n",
    "            if any([e['error_type'] != None and e['error_type'] != Error.BAD_DELETION for e in s['processed_annotations']]):\n",
    "                count += 1\n",
    "    else:\n",
    "        for s in data:\n",
    "            if any([e['error_type'] != None for e in s['processed_annotations']]):\n",
    "                count += 1\n",
    "    return count / len(data)\n",
    "\n",
    "\n",
    "all_edits = [i for j in [s['processed_annotations'] for s in data] for i in j]\n",
    "error_edits = [e for e in all_edits if e['error_type'] != None]\n",
    "\n",
    "print(f'% all edits that are errors: {(len(error_edits)/len(all_edits)):.2f}')\n",
    "print(f'Error rate for all systems: {error_rate(data):.2f}')\n",
    "print(f'Error rate for MUSS: {error_rate(muss):.2f}')\n",
    "print(f'Error rate for GPT-zero: {error_rate(gpt_zero):.2f}')\n",
    "print(f'Error rate for human/GPT-few: {error_rate(human):.2f} / {error_rate(gpt_few):.2f}')\n",
    "print(f'Error rate for human/GPT-few excluding bad deletion: {error_rate(human, exclude_bad_deletion=True):.2f} / {error_rate(gpt_few, exclude_bad_deletion=True):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tmp(Enum):\n",
    "    NO_ERROR = 'No Error'\n",
    "\n",
    "cust_systems = [s for s in systems if 'Human' not in s] + ['aggregated/human']\n",
    "\n",
    "fig, ax = plt.subplots(1, len(cust_systems), figsize=(20, 3), sharey=True, gridspec_kw = {'wspace': 0.1, 'hspace':0})\n",
    "for i, system in enumerate([s for s in all_system_labels if s in cust_systems]):\n",
    "    selected = [sent for sent in data if sent['system'] == system]\n",
    "    if system == 'aggregated/human':\n",
    "        selected = [sent for sent in data if 'Human' in sent['system']]\n",
    "\n",
    "    error_segmentation = {}\n",
    "    for error in Error:\n",
    "        error_segmentation[error] = [sent for sent in selected if any([ann['error_type'] == error for ann in sent['processed_annotations']])]\n",
    "    error_segmentation[Tmp.NO_ERROR] = [sent for sent in selected if not any([ann['error_type'] == error for ann in sent['processed_annotations']])]\n",
    "\n",
    "    pts = []\n",
    "    pts += [(error, avg([s['score'] for s in error_segmentation[error]])) for error in error_segmentation.keys()]\n",
    "    pts = sorted([p for p in pts if p[1] != 0], key=lambda x: x[1])\n",
    "\n",
    "    cust_mapping = color_mapping\n",
    "    cust_mapping[Tmp.NO_ERROR] = '#64C466'\n",
    "\n",
    "    ax[i].axhline(0, linestyle='-', color='black', linewidth=0.3) \n",
    "\n",
    "    ax[i].bar([p[0].value for p in pts], [p[1] for p in pts], width, color=[cust_mapping[label] for label in [p[0] for p in pts]])\n",
    "    ax[i].set_title(system_name_mapping[system])\n",
    "    # ax[i].set_xticks([])\n",
    "    ax[i].set_yticks(np.arange(-2, 2.5, 0.5))\n",
    "\n",
    "\n",
    "for tick in [i for j in [x.get_xticklabels() for x in ax] for i in j]:\n",
    "    tick.set_rotation(45)\n",
    "    tick.set_horizontalalignment('right')\n",
    "\n",
    "# fig.suptitle('Avg. Error Sent Scores')\n",
    "ax[0].set_ylabel('Our Score')\n",
    "# plt.legend([p[0].value for p in pts], loc='center left', bbox_to_anchor=(1.5, 0.5), ncol=3)\n",
    "out_filename = f'../paper/plot/appendix/error-scores.pdf'\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Length of Quality / Error Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = [\n",
    "    'elaboration',\n",
    "    'generalization',\n",
    "    'paraphrase',\n",
    "    'split',\n",
    "    'reorder',\n",
    "    'structure',\n",
    "]\n",
    "\n",
    "class Tmp(Enum):\n",
    "    NO_ERROR = 'No Error'\n",
    "\n",
    "import scipy.stats as st\n",
    "def ci(int_data):\n",
    "    y1, y2 = st.t.interval(alpha=0.95, df=len(int_data)-1, loc=np.mean(int_data), scale=st.sem(int_data)) \n",
    "    # If it's too big, we just won't even return it\n",
    "    if y1 <= 0.01 or math.isnan(y1):\n",
    "        return 0\n",
    "    return (y2-y1) / 2\n",
    "\n",
    "width = 0.65\n",
    "annotations = [x for y in [sent['processed_annotations'] for sent in data] for x in y]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 4))\n",
    "\n",
    "fam_size = [(family, avg([x['size'] for x in get_annotations_by_edit_family(data, family)]), ci([x['size'] for x in get_annotations_by_edit_family(data, family)])) for family in families]\n",
    "pts = sorted([p for p in fam_size if p[1] != 0], key=lambda x: x[1], reverse=True)\n",
    "ax[0].bar([p[0].capitalize() for p in pts], [p[1] for p in pts], width, yerr=[p[2] for p in pts], color=[color_mapping[label] for label in [p[0] for p in pts]])\n",
    "\n",
    "anns = [ann for sent in data for ann in sent['processed_annotations']]\n",
    "error_segmentation = {}\n",
    "for error in Error:\n",
    "    error_segmentation[error] = [a for a in anns if error == a['error_type']]\n",
    "error_segmentation[Tmp.NO_ERROR] = [a for a in anns if error == None]\n",
    "pts = [(error, avg([s['size'] for s in error_segmentation[error]]), ci([s['size'] for s in error_segmentation[error]])) for error in error_segmentation.keys()]\n",
    "pts = sorted([p for p in pts if p[1] != 0], key=lambda x: x[1], reverse=True)\n",
    "ax[1].bar([p[0].value for p in pts], [p[1] for p in pts], width, yerr=[p[2] for p in pts], color=[color_mapping[label] for label in [p[0] for p in pts]])\n",
    "\n",
    "# ax[0].set_xlabel('Edit Family')\n",
    "# ax[1].set_xlabel('Error Type')\n",
    "\n",
    "ax[0].set_ylabel('Avg. Char Length')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "# plt.suptitle('Avg. Edit Span Size by Type')\n",
    "# ax[1].set_title('Avg. Error Span Size')\n",
    "\n",
    "ax[0].set_yticks(np.arange(0, 0.085, 0.02))\n",
    "ax[1].set_yticks(np.arange(0, 0.25, 0.04))\n",
    "\n",
    "for tick in ax[0].get_xticklabels() + ax[1].get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    tick.set_horizontalalignment('right')\n",
    "\n",
    "out_filename = f'../paper/plot/appendix/edit-sizes.pdf'\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-level Scores Between Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# & -- & -- & -- & -- & -- & -- & -- & -- \\tabularnewline\n",
    "\n",
    "# Print the avg. sentence scores for each system\n",
    "table = ''\n",
    "\n",
    "subscores = ['lexical', 'syntax', 'content', 'error', 'quality']\n",
    "\n",
    "std = lambda x: round(np.std(x, ddof=1), 2)\n",
    "\n",
    "for system in [s for s in all_system_labels if s in systems and 'Human' not in s] + ['aggregated/human']:\n",
    "    selected = [sent for sent in data if sent[\"system\"] == system]\n",
    "    if system == 'aggregated/human':\n",
    "        selected = [sent for sent in data if 'Human' in sent['system']]\n",
    "\n",
    "    line = f'{system_name_mapping[system]} & '\n",
    "\n",
    "    for score in subscores:\n",
    "        selected_scores = [s[\"subscores\"][score] for s in selected]\n",
    "        line += f'{avg(selected_scores, 2):.2f} & {std(selected_scores):.2f} & '\n",
    "\n",
    "    selected_scores = [s[\"score\"] for s in selected]\n",
    "    line += f'{avg(selected_scores, 2):.2f} & {std(selected_scores):.2f}'\n",
    "\n",
    "    table += f'{line} \\\\tabularnewline\\n'\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length Impacts Edit Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotated ASSET data\n",
    "import utils.dataloader_old as dl\n",
    "asset = dl.load_data('../data/annotated', batch_num=[1, 2, 3, 4], preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in data:\n",
    "    sent['ed'] = edit_dist(sent['original'], sent['simplified'])\n",
    "\n",
    "for sent in asset:\n",
    "    sent['ed'] = edit_dist(sent['original'], sent['simplified'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 300\n",
    "\n",
    "pts = []\n",
    "for sent in data:\n",
    "    pts += [(\n",
    "        len(sent['processed_annotations']),\n",
    "        sent['ed']\n",
    "    )]\n",
    "pts = random.sample(pts, n)\n",
    "x, y = [p[0] for p in pts], [p[1] for p in pts]\n",
    "plt.scatter(x, y, color='red', label='SimpEval', marker=\"*\", alpha=0.5)\n",
    "\n",
    "# a, b = np.polyfit(x, y, 1)\n",
    "# plt.plot(x, a*np.asarray(x)+b, color='red', alpha=0.5)\n",
    "\n",
    "pts = []\n",
    "for sent in asset:\n",
    "    pts += [(\n",
    "        len(sent['processed_annotations']),\n",
    "        sent['ed']\n",
    "    )]\n",
    "pts = random.sample(pts, n)\n",
    "x, y = [p[0] for p in pts], [p[1] for p in pts]\n",
    "plt.scatter(x, y, color='blue', label='ASSET', marker=\"P\", alpha=0.5)\n",
    "\n",
    "# a, b = np.polyfit(x, y, 1)\n",
    "# plt.plot(x, a*np.asarray(x)+b, color='blue', alpha=0.5)\n",
    "\n",
    "plt.xticks(np.arange(0, 27, 1))\n",
    "\n",
    "# plt.title('Edit Distance vs. Number of Edits')\n",
    "plt.xlabel('Number of Edits')\n",
    "plt.ylabel('Edit Distance')\n",
    "plt.legend()\n",
    "out_filename = f'../paper/plot/appendix/edit-distance-num-edits.pdf'\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Split Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(5, 5), sharey=True)\n",
    "\n",
    "# If a bucket has less than this value, we don't include it\n",
    "representative_sample_size = 25\n",
    "\n",
    "for k, dataset in enumerate([data, asset]):\n",
    "    bucket_size = 20\n",
    "    start_amt = 0\n",
    "    end_amt = 205\n",
    "    pts = {}\n",
    "    for i in range((end_amt - start_amt) // bucket_size):\n",
    "        start_size = start_amt + i * bucket_size\n",
    "        end_size = start_amt + (i + 1) * bucket_size\n",
    "\n",
    "        selected = [s for s in dataset if start_size <= s['ed'] < end_size]\n",
    "\n",
    "        amt_splits = {}\n",
    "        for amt in range(1, 4):\n",
    "            if amt not in amt_splits.keys():\n",
    "                amt_splits[amt] = 0\n",
    "            for sent in selected:\n",
    "                if len([a for a in sent['edits'] if a['type'] == 'split']) == amt:\n",
    "                    amt_splits[amt] += 1\n",
    "        \n",
    "        for amt in range(1, 4):\n",
    "            if amt not in pts.keys():\n",
    "                pts[amt] = []\n",
    "            if len(selected) > representative_sample_size:\n",
    "                pts[amt] += [(start_size, amt_splits[amt] / len(selected))]\n",
    "            else:\n",
    "                pts[amt] += [(start_size, 0)]\n",
    "    bottom = [0 for _ in range(len(pts[1])-1)]\n",
    "    for amt in range(1, 4):\n",
    "        val = [p[1] for p in pts[amt][1:]]\n",
    "        ax[k, 0].hist([p[0] for p in pts[amt]][:-1], [p[0] for p in pts[amt]], \n",
    "            bottom=bottom, weights=[p[1] for p in pts[amt][1:]], \n",
    "            color=color_mapping[f'split-{amt}'], edgecolor='black', linewidth=1.2)\n",
    "        bottom = [b + p for b, p in zip(bottom, val)]\n",
    "\n",
    "\n",
    "    bucket_size = 20\n",
    "    start_amt = 75\n",
    "    end_amt = 300\n",
    "    pts = {}\n",
    "    for i in range((end_amt - start_amt) // bucket_size):\n",
    "        start_size = start_amt + i * bucket_size\n",
    "        end_size = start_amt + (i + 1) * bucket_size\n",
    "\n",
    "        selected = [s for s in dataset if start_size <= len(s['simplified']) < end_size]\n",
    "\n",
    "        amt_splits = {}\n",
    "        for amt in range(1, 4):\n",
    "            if amt not in amt_splits.keys():\n",
    "                amt_splits[amt] = 0\n",
    "            for sent in selected:\n",
    "                if len([a for a in sent['edits'] if a['type'] == 'split']) == amt:\n",
    "                    amt_splits[amt] += 1\n",
    "        \n",
    "        for amt in range(1, 4):\n",
    "            if amt not in pts.keys():\n",
    "                pts[amt] = []\n",
    "            if len(selected) > representative_sample_size:\n",
    "                pts[amt] += [(start_size, amt_splits[amt] / len(selected))]\n",
    "            else:\n",
    "                pts[amt] += [(start_size, 0)]\n",
    "    bottom = [0 for _ in range(len(pts[1])-1)]\n",
    "    for amt in range(1, 4):\n",
    "        val = [p[1] for p in pts[amt][1:]]\n",
    "        ax[k, 1].hist([p[0] for p in pts[amt]][:-1], [p[0] for p in pts[amt]], \n",
    "            bottom=bottom, weights=[p[1] for p in pts[amt][1:]], \n",
    "            color=color_mapping[f'split-{amt}'], edgecolor='black', linewidth=1.2)\n",
    "        bottom = [b + p for b, p in zip(bottom, val)]\n",
    "\n",
    "# fig.suptitle('Proportion of Sentences with a Split Edit')\n",
    "ax[0, 0].set_xticks([])\n",
    "ax[0, 1].set_xticks([])\n",
    "ax[1, 0].set_xlabel('Edit Distance')\n",
    "ax[1, 1].set_xlabel('Original Sentence Length')\n",
    "ax[0, 0].set_ylabel('% SimpEval w/ Split')\n",
    "ax[1, 0].set_ylabel('% ASSET w/ Split')\n",
    "ax[0, 0].set_yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "fig.legend(labels=[\n",
    "        '1 Split',\n",
    "        '2 Splits',\n",
    "        '3 Splits',\n",
    "    ], loc='lower center', bbox_to_anchor=(0.54, -0.06), framealpha=1, frameon=False,\n",
    "    handlelength=1, handleheight=1, ncol=3, handletextpad=0.5, columnspacing=0.7\n",
    ")\n",
    "\n",
    "out_filename = f'../paper/plot/appendix/split-edit-sizes.pdf'\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SALSA Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quality_Condition(Enum):\n",
    "    ELABORATION='Elaboration'\n",
    "    GENERALIZATION='Generalization'\n",
    "    WORD_REORDER='Word-level Reorder'\n",
    "    COMPONENT_REORDER='Component-level Reorder'\n",
    "    SPLIT='Sentence Split'\n",
    "    STRUCTURE='Structure Change'\n",
    "    PARAPHRASE='Paraphrase'\n",
    "\n",
    "class Error_Condition(Enum):\n",
    "    BAD_DELETION='Bad Deletion'\n",
    "    COREFERENCE='Coreference'\n",
    "    REPETITION='Repetition'\n",
    "    CONTRADICTION='Contradiction'\n",
    "    FACTUAL_ERROR='Factual Error'\n",
    "    IRRELEVANT='Irrelevant'\n",
    "    BAD_WORD_REORDER='Bad Word-level Reorder'\n",
    "    BAD_COMPONENT_REORDER='Bad Component-level Reorder'\n",
    "    BAD_STRUCTURE='Bad Structure Change'\n",
    "    BAD_SPLIT='Bad Sentence Split'\n",
    "    COMPLEX_WORDING='Complex Wording'\n",
    "    INFORMATION_REWRITE='Information Rewrite'\n",
    "    GRAMMAR='Grammar Error'\n",
    "\n",
    "class Trivial_Condition(Enum):\n",
    "    TRIVIAL=\"Trivial Insertion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edits_from_condition(edits, condition):\n",
    "    if condition == Quality_Condition.ELABORATION:\n",
    "        return [e for e in edits if \n",
    "            e['information_impact'] == Information.MORE and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    elif condition == Quality_Condition.GENERALIZATION:\n",
    "        return [e for e in edits if \n",
    "            e['information_impact'] == Information.LESS and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    elif condition == Quality_Condition.PARAPHRASE:\n",
    "        return [e for e in edits if \n",
    "            e['information_impact'] == Information.SAME and\n",
    "            e['edit_type'] == 'substitution' and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    \n",
    "    elif condition == Quality_Condition.SPLIT:\n",
    "        return [e for e in edits if \n",
    "            e['edit_type'] == 'split' and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    \n",
    "    elif condition == Quality_Condition.WORD_REORDER:\n",
    "        return [e for e in edits if \n",
    "            e['edit_type'] == 'reorder' and\n",
    "            e['reorder_level'] == ReorderLevel.WORD and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    \n",
    "    elif condition == Quality_Condition.COMPONENT_REORDER:\n",
    "        return [e for e in edits if \n",
    "            e['edit_type'] == 'reorder' and\n",
    "            e['reorder_level'] == ReorderLevel.COMPONENT and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "    \n",
    "    elif condition == Quality_Condition.STRUCTURE:\n",
    "        return [e for e in edits if \n",
    "            e['edit_type'] == 'structure' and\n",
    "            e['type'] == Quality.QUALITY\n",
    "        ]\n",
    "\n",
    "    if condition == Trivial_Condition.TRIVIAL:\n",
    "        return [e for e in edits if \n",
    "            e['type'] == Quality.TRIVIAL\n",
    "        ]\n",
    "\n",
    "    edits = [e for e in edits if e['type'] == Quality.ERROR]\n",
    "\n",
    "    if condition == Error_Condition.CONTRADICTION:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.CONTRADICTION\n",
    "        ]\n",
    "    elif condition == Error_Condition.FACTUAL_ERROR:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.FACTUAL\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.IRRELEVANT:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.IRRELEVANT\n",
    "        ]\n",
    "\n",
    "    elif condition == Error_Condition.REPETITION:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.REPETITION\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.BAD_DELETION:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.BAD_DELETION\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.COREFERENCE:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.COREFERENCE\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.INFORMATION_REWRITE:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.INFORMATION_REWRITE\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.BAD_SPLIT:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.BAD_SPLIT\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.BAD_WORD_REORDER:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.BAD_REORDER and\n",
    "            e['reorder_level'] == ReorderLevel.WORD \n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.BAD_COMPONENT_REORDER:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.BAD_REORDER and\n",
    "            e['reorder_level'] == ReorderLevel.COMPONENT \n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.BAD_STRUCTURE:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.BAD_STRUCTURE\n",
    "        ]\n",
    "        \n",
    "    elif condition == Error_Condition.COMPLEX_WORDING:\n",
    "        return [e for e in edits if \n",
    "            e['error_type'] == Error.COMPLEX_WORDING\n",
    "        ]        \n",
    "    elif condition == Error_Condition.GRAMMAR:\n",
    "        return [e for e in edits if \n",
    "            e['grammar_error'] == True\n",
    "        ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('../data/inspection_rating_annotated', preprocess=True, adjudicated=True)\n",
    "test_data = load_data('../data/test_set_inspection_rating_annotated', preprocess=True, adjudicated=True)\n",
    "data = train_data + test_data\n",
    "\n",
    "all_edits = [i for j in [s['processed_annotations'] for s in data] for i in j]\n",
    "print(f\"Total number of edits: {len(all_edits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ''\n",
    "\n",
    "# Quality edits\n",
    "table += '\\\\multirow{7}{*}{\\\\rotatebox[origin=c]{90}{Quality Evaluation}}'\n",
    "for condition in Quality_Condition:\n",
    "    edits = get_edits_from_condition(all_edits, condition)\n",
    "    table += f' & {condition.value} & {len(edits)} & {sum([x[\"token_size\"] for x in edits])} & {avg([x[\"rating\"] for x in edits], prec=2)} \\\\\\\\\\n'\n",
    "\n",
    "table += '\\\\\\midrule\\n\\\\multirow{15}{*}{\\\\rotatebox[origin=c]{90}{Error Evaluation}}'\n",
    "\n",
    "for condition in Error_Condition:\n",
    "    edits = get_edits_from_condition(all_edits, condition)\n",
    "    table += f' & {condition.value} & {len(edits)} & {sum([x[\"token_size\"] for x in edits])} & {-avg([x[\"rating\"] for x in edits if x[\"rating\"] is not None], prec=2)} \\\\\\\\\\n'\n",
    "\n",
    "edits = get_edits_from_condition(all_edits, Trivial_Condition.TRIVIAL)\n",
    "table += f'\\\\\\midrule\\n & Trivial Change & {len(edits)} & {sum([x[\"token_size\"] for x in edits])} & {0} \\\\\\\\\\n'\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
