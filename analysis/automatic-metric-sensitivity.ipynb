{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "data = load_data('../data/salsa_test.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from utils.sari import *\n",
    "import bert_score as bs\n",
    "from bert_score import BERTScorer\n",
    "from utils.util import avg\n",
    "\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "comet_model_path = comet.download_model('wmt21-comet-mqm')\n",
    "comet_mqm = comet.load_from_checkpoint(comet_model_path)\n",
    "\n",
    "def BLEUsent(hypothesis, references):\n",
    "    ref = [x.split(' ') for x in references]\n",
    "    hyp = hypothesis.split(' ')\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu(ref, hyp)\n",
    "    return BLEUscore\n",
    "\n",
    "def BERTSCOREsent(hypothesis, references):\n",
    "    hypothesis_all = [hypothesis for _ in range(len(references))]\n",
    "    _, _, F1 = scorer.score(hypothesis_all, references)\n",
    "    return avg(F1.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set: patch with external human references\n",
    "file_path = '../data/test_set_human_written.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    human_references = f.read().split('\\n\\n')\n",
    "    human_references = [sent.split('\\n') for sent in human_references]\n",
    "    human_references = [{\n",
    "        'original': sent[0],\n",
    "        'simplified': sent[1]\n",
    "    } for sent in human_references]\n",
    "\n",
    "# Create a scores list with references and simplifications\n",
    "scores = []\n",
    "for orig in set([x['original'] for x in data]):\n",
    "    sents = [sent for sent in data if sent['original'] == orig]\n",
    "\n",
    "    humans = [sent for sent in human_references if sent['original'] == orig]\n",
    "    assert len(humans) == 2\n",
    "    \n",
    "    systems = [sent for sent in sents if 'Human' not in sent['system']]\n",
    "\n",
    "    if len(systems) == 0:\n",
    "        continue\n",
    "\n",
    "    for system in systems:\n",
    "        references = list(set([sent['simplified'] for sent in humans]))\n",
    "        prediction = system['simplified']\n",
    "        \n",
    "        # if system['simpeval_scores'] is None:\n",
    "        #     continue\n",
    "        # simpeval_score = avg(system['simpeval_scores'])\n",
    "\n",
    "        score = {\n",
    "            'original': orig,\n",
    "            'simplified': prediction,\n",
    "            'references': references,\n",
    "            'system': system['system'],\n",
    "            # 'simpeval': simpeval_score,\n",
    "        }\n",
    "\n",
    "        scores += [score]\n",
    "    \n",
    "    # Add human written outputs with other human output as reference\n",
    "    # for human in humans:\n",
    "    #     reference = list(set([sent['simplified'] for sent in humans if sent['simplified'] != human['simplified']]))\n",
    "    #     scores += [{\n",
    "    #         'original': orig,\n",
    "    #         'simplified': human['simplified'],\n",
    "    #         'references': reference,\n",
    "    #         'system': human['system'],\n",
    "    #     }]\n",
    "\n",
    "print(f\"Calculating sensitivity with scores for {len(scores)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train set: create a scores list with references and simplifications\n",
    "scores = []\n",
    "for orig in set([x['original'] for x in data]):\n",
    "    sents = [sent for sent in data if sent['original'] == orig]\n",
    "\n",
    "    humans = [sent for sent in sents if 'Human' in sent['system']]\n",
    "    systems = [sent for sent in sents if 'Human' not in sent['system']]\n",
    "\n",
    "    if len(systems) == 0:\n",
    "        continue\n",
    "\n",
    "    for system in systems:\n",
    "        references = list(set([sent['simplified'] for sent in humans]))\n",
    "        prediction = system['simplified']\n",
    "        \n",
    "        # if system['simpeval_scores'] is None:\n",
    "        #     continue\n",
    "        # simpeval_score = avg(system['simpeval_scores'])\n",
    "\n",
    "        score = {\n",
    "            'original': orig,\n",
    "            'simplified': prediction,\n",
    "            'references': references,\n",
    "            'system': system['system'],\n",
    "            # 'simpeval': simpeval_score,\n",
    "        }\n",
    "\n",
    "        scores += [score]\n",
    "    \n",
    "    # Add human written outputs with other human output as reference\n",
    "    for human in humans:\n",
    "        reference = list(set([sent['simplified'] for sent in humans if sent['simplified'] != human['simplified']]))\n",
    "        scores += [{\n",
    "            'original': orig,\n",
    "            'simplified': human['simplified'],\n",
    "            'references': reference,\n",
    "            'system': human['system'],\n",
    "        }]\n",
    "\n",
    "print(f\"Calculating sensitivity with scores for {len(scores)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calculated scores\n",
    "for score in scores:    \n",
    "    original, prediction, references = score['original'], score['simplified'], score['references']\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    try:\n",
    "        score['bleu'] = BLEUsent(prediction, references)\n",
    "    except Exception:\n",
    "        print(\"Skipping corrput sentence...\")\n",
    "        continue\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    score['bertscore'] = BERTSCOREsent(prediction, references)\n",
    "\n",
    "    # Calculate SARI\n",
    "    score['sari_add'], score['sari_keep'], score['sari_del'], score['sari'] = SARIsent(original, prediction, references, components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write scoring setup to json to add COMET scores\n",
    "with open('../lens/1-scores-no-comet-lens.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add COMET scores\n",
    "with open('../lens/2-scores-comet-only.json', 'r') as f:\n",
    "    comet_results = json.load(f)\n",
    "\n",
    "for score in scores:\n",
    "    # Ensure both the system and sentence is the same\n",
    "    aligned = [sent for sent in comet_results if \n",
    "        sent['original'] == score['original'] and\n",
    "        sent['system'] == score['system']\n",
    "    ]\n",
    "    \n",
    "    if len(aligned) == 0 or 'comet' not in aligned[0].keys():\n",
    "        score['comet'] = 0\n",
    "        continue\n",
    "    \n",
    "    comet_score = aligned[0]['comet']\n",
    "    score['comet'] = comet_score\n",
    "\n",
    "with open('../lens/3-scores-no-lens.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LENS scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our scores\n",
    "conditions = [\n",
    "    'quality_content', \n",
    "    'quality_syntax', \n",
    "    'quality_lexical', \n",
    "    'error_content', \n",
    "    'error_syntax', \n",
    "    'error_lexical', \n",
    "    'quality', \n",
    "    'error', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "for s in scores:\n",
    "    s['system'] = s['system'] \\\n",
    "        .replace('simpeval-22', 'new-wiki-1') \\\n",
    "        .replace('simpeval-ext', 'new-wiki-1')\n",
    "\n",
    "# Create a scores list with references and simplifications\n",
    "our_scores = []\n",
    "for orig in set([x['original'] for x in data]):\n",
    "    sents = [sent for sent in data if sent['original'] == orig]\n",
    "\n",
    "    human = [sent for sent in sents if 'Human' in sent['system']]\n",
    "    systems = [sent for sent in sents if 'Human' not in sent['system']] # Or you can try just GPT-3-Few\n",
    "\n",
    "    if len(systems) == 0:\n",
    "        continue\n",
    "\n",
    "    for system in systems:\n",
    "        # print(system['system'])\n",
    "        # print(score['system'])\n",
    "        aligned = [sent for sent in scores if \n",
    "            sent['original'] == orig and\n",
    "            sent['system'] in system['system'] \n",
    "        ]\n",
    "\n",
    "        if len(aligned) == 0:\n",
    "            continue\n",
    "            \n",
    "        n_score = aligned[0]\n",
    "        \n",
    "        n_score['our_score'] = system['score']\n",
    "        for condition in conditions:\n",
    "            n_score[f'our_score_{condition}'] = calculate_sentence_score(system, get_params(condition))\n",
    "            # if 'error' in condition:\n",
    "            #     n_score[f'our_score_{condition}'] = -n_score[f'our_score_{condition}']\n",
    "\n",
    "        our_scores += [n_score]\n",
    "\n",
    "print(f\"Calculating sensitivity with scores for {len(our_scores)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_name_mapping = {\n",
    "    'quality_lexical': 'Lexical',\n",
    "    'quality_syntax': 'Syntax',\n",
    "    'quality_content': 'Conceptual',\n",
    "    'error_lexical': 'Lexical',\n",
    "    'error_syntax': 'Syntax',\n",
    "    'error_content': 'Conceptual',\n",
    "    'error': 'All Error',\n",
    "    'quality': 'All Quality',\n",
    "    'all': 'All Edits'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Kendall Tau correlation for each statistic\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import heapq\n",
    "metrics = ['bleu', 'sari', 'bertscore', 'comet', 'lens']\n",
    "all_results = []\n",
    "prec = 3\n",
    "\n",
    "# Calculate metric corrleation\n",
    "for metric in metrics:\n",
    "    sys_results = []\n",
    "    for condition in condition_name_mapping.keys():\n",
    "        p = pearsonr(\n",
    "            [s[f'our_score_{condition}'] for s in scores if s[metric] is not None], \n",
    "            [s[metric] for s in scores if s[metric] is not None]\n",
    "        )\n",
    "        sp = spearmanr(\n",
    "            [s[f'our_score_{condition}'] for s in scores if s[metric] is not None], \n",
    "            [s[metric] for s in scores if s[metric] is not None]\n",
    "        )\n",
    "        results = (round(sp[0], prec), round(p[0], prec))\n",
    "        sys_results += [results]\n",
    "    all_results += [sys_results]\n",
    "\n",
    "# Render LaTeX table\n",
    "delimiters = [\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{Quality}}',\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{Error}}',\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{All}}'\n",
    "]\n",
    "\n",
    "out = ''\n",
    "for i, condition in enumerate(condition_name_mapping.keys()):\n",
    "    line = ''\n",
    "\n",
    "    if i % 3 == 0:\n",
    "        if i != 0:\n",
    "            line += '\\\\midrule\\n'\n",
    "        line += f'{delimiters[int(i/3)]} '\n",
    "        \n",
    "    line += f'& {condition_name_mapping[condition]} & '\n",
    "    a_max_p, b_max_p = heapq.nlargest(2, [x[i][0] for x in all_results])\n",
    "    a_max_sp, b_max_sp = heapq.nlargest(2, [x[i][1] for x in all_results])\n",
    "    for j, metric in enumerate(metrics):\n",
    "        p = all_results[j][i][0]\n",
    "        sp = all_results[j][i][1]\n",
    "\n",
    "        if str(p) == 'nan':\n",
    "            p = '---'\n",
    "        if str(sp) == 'nan':\n",
    "            sp = '---'\n",
    "\n",
    "        if p == a_max_p:\n",
    "            p = f'\\\\textbf{{{round(p, prec):.3f}}}'\n",
    "        elif sp == a_max_sp:\n",
    "            sp = f'\\\\textbf{{{round(sp, prec):.3f}}}'\n",
    "        elif p == b_max_p:\n",
    "            p = f'\\\\underline{{{round(p, prec):.3f}}}'\n",
    "        elif sp == b_max_sp:\n",
    "            sp = f'\\\\underline{{{round(sp, prec):.3f}}}'\n",
    "        else:\n",
    "            p = f'{round(p, prec):.3f}'\n",
    "\n",
    "        # line += f'{p} & {sp} & '\n",
    "        line += f'{p} & '\n",
    "    out += line[:-2] + '\\\\tabularnewline\\n'\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
