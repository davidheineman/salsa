{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from utils.dataloader_legacy import load_data\n",
    "\n",
    "data = load_data('../data/salsa-non-adjudicated/salsa_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "def token_level_disagreement(data, collapse_composite=False, majority_disagreement=False):\n",
    "    total_agreement = []\n",
    "    MAX_HIT = max([x['id'] for x in data])+1\n",
    "    for sent_id in range(0, MAX_HIT):\n",
    "        sents = [x for x in data if x['id'] == sent_id]\n",
    "        orig_tokens = get_annotations_per_token(sents, 'original', collapse_composite=collapse_composite)\n",
    "        simp_tokens = get_annotations_per_token(sents, 'simplified', collapse_composite=collapse_composite)\n",
    "\n",
    "        arr_edits = []\n",
    "        for edits in list(orig_tokens.values()) + list(simp_tokens.values()):\n",
    "            out = []\n",
    "            for k, v in edits.items():\n",
    "                for amt in range(v):\n",
    "                    out += [k]\n",
    "            for i in range(3 - len(out)):\n",
    "                out += [None]\n",
    "            arr_edits += [out]\n",
    "\n",
    "        if majority_disagreement:\n",
    "            # This takes edits with a 2 agreeing annotators and 1 disagreeing annotator\n",
    "            # and reports agreement between the majority and minority classes\n",
    "            disagreement = []\n",
    "            for edit in arr_edits:\n",
    "                majority, minority = None, None\n",
    "                if edit[0] == edit[1]:\n",
    "                    majority = edit[0]\n",
    "                    minority = edit[2]\n",
    "                elif edit[1] == edit[2]:\n",
    "                    majority = edit[1]\n",
    "                    minority = edit[0]\n",
    "                elif edit[0] == edit[2]:\n",
    "                    majority = edit[0]\n",
    "                    minority = edit[1]\n",
    "                \n",
    "                if majority is None or minority is None or majority == minority:\n",
    "                    continue\n",
    "            \n",
    "                disagreement += [[majority, minority]]\n",
    "        else:\n",
    "            # This replicates each edit and calculates pairwise agreement\n",
    "            # [A1, A2]\n",
    "            # [A2, A3]\n",
    "            # [A3, A1]\n",
    "            interwoven = []\n",
    "            for edit in arr_edits:\n",
    "                interwoven += [[edit[0], edit[1]]]\n",
    "                interwoven += [[edit[1], edit[2]]]\n",
    "                interwoven += [[edit[0], edit[2]]]\n",
    "            \n",
    "            disagreement = []\n",
    "            for edit in interwoven:\n",
    "                # Important point, could be 'or'\n",
    "                if edit[0] is None or edit[1] is None:\n",
    "                    continue\n",
    "                # Cannot display agreement\n",
    "                if edit[0] == edit[1]:\n",
    "                    continue\n",
    "                disagreement += [edit]\n",
    "\n",
    "        total_agreement += disagreement\n",
    "    return total_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5)) #sharex=True, sharey=True)\n",
    "\n",
    "vmin, vmax = 0, 2500\n",
    "\n",
    "color_bar = sn.color_palette(\"Reds\", 20)\n",
    "\n",
    "# Change label ordering\n",
    "edit_type_labels = ['insertion', 'deletion', 'substitution', 'reorder', 'split', 'structure']\n",
    "\n",
    "disagreement = token_level_disagreement(data)\n",
    "a = confusion_matrix([x[0] for x in disagreement], [x[1] for x in disagreement], labels=edit_type_labels)\n",
    "df_cm = pd.DataFrame(a, index=[x.capitalize() for x in edit_type_labels], columns=[x.capitalize() for x in edit_type_labels])\n",
    "sn.heatmap(df_cm, cmap=color_bar, ax=ax1, vmin=vmin, vmax=vmax, square=True, cbar=False, linewidth=.5)\n",
    "\n",
    "disagreement = token_level_disagreement(data, collapse_composite=True)\n",
    "a = confusion_matrix([x[0] for x in disagreement], [x[1] for x in disagreement], labels=[x for x in edit_type_labels if x not in ['structure', 'split']])\n",
    "df_cm = pd.DataFrame(a, index=[x.capitalize() for x in edit_type_labels if x not in ['structure', 'split']], columns=[x.capitalize() for x in edit_type_labels if x not in ['structure', 'split']])\n",
    "sn.heatmap(df_cm, cmap=color_bar, ax=ax2, vmin=vmin, vmax=vmax, square=True, linewidth=.5)\n",
    "\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "ax1.set_title('Using composite edits', pad=12, fontsize=14)\n",
    "ax2.set_title('Using constituent edits', pad=12, fontsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "out_filename = \"../paper/plot/agreement-heatmap.pdf\"\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Rough Span Agreement Score\n",
    "# TODO: Does not work for multiple batches\n",
    "total_agreement = []\n",
    "MAX_HIT = max([x['id'] for x in data])+1\n",
    "for sent_id in range(0, MAX_HIT):\n",
    "    sents = [x for x in data if x['id'] == sent_id]\n",
    "    orig_tokens = get_annotations_per_token(sents, 'original')\n",
    "    simp_tokens = get_annotations_per_token(sents, 'simplified')\n",
    "    agg_score = 0\n",
    "    for val in list(orig_tokens.values()) + list(simp_tokens.values()):\n",
    "        agg_score += (list(val.values())[0] - 3)\n",
    "    total_agreement.append(agg_score)\n",
    "print(f\"Sent with full agreement {sum([x == 0 for x in total_agreement])} / {MAX_HIT}\")\n",
    "# hds = sorted([(i, val) for i, val in enumerate(total_agreement)], key=lambda x: x[1])\n",
    "# print(\"\\nHighest disagreement sentences:\")\n",
    "# for x in [get_sent_info(data[x[0]]) for x in hds][:5]:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_order_annotations_per_token(sents, sent_type, remove_none=True):\n",
    "    edit_dict_value = sent_type + '_span'\n",
    "    tokens = generate_token_dict(sents[0][sent_type])\n",
    "    \n",
    "    # Iterate through all annotators' edits\n",
    "    for sent in sents:\n",
    "        edits = sent['edits']\n",
    "        \n",
    "        for edit in edits:\n",
    "            if edit[edit_dict_value] is None:\n",
    "                continue\n",
    "\n",
    "            for elongated_span in edit[edit_dict_value]:\n",
    "                composite_spans = [\n",
    "                    (entry[0] + elongated_span[0], entry[1] + elongated_span[0]) \n",
    "                    for entry in list(\n",
    "                        generate_token_dict(sents[0][sent_type][elongated_span[0]:elongated_span[1]]).keys()\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                for c_span in composite_spans:\n",
    "                    if c_span in tokens.keys():                       \n",
    "                        type_ = edit['type']\n",
    "\n",
    "                        # Extract subtype from edit entry\n",
    "                        if type_ == 'substitution':\n",
    "                            subtype = edit['annotation'][0]\n",
    "                            if subtype == 'more':\n",
    "                                type_ = 'elaboration_substitution'\n",
    "                            elif subtype == 'less':\n",
    "                                type_ = 'generalization_substitution'\n",
    "                            elif subtype == 'same':\n",
    "                                type_ = 'paraphrasing_substitution'\n",
    "                            elif subtype == 'different':\n",
    "                                type_ = 'paraphrasing_different'\n",
    "                            else:\n",
    "                                print(edit)\n",
    "                        elif type_ == 'reorder':\n",
    "                            subtype = edit['annotation'][-1]\n",
    "                            if subtype == 'word':\n",
    "                                type_ = 'word_reorder'\n",
    "                            elif subtype == 'component':\n",
    "                                type_ = 'component_reorder'\n",
    "                            else:\n",
    "                                continue\n",
    "                            # If the annotator does not specify, we skip\n",
    "\n",
    "                        # Increment the edit type\n",
    "                        if type_ not in tokens[c_span].keys():\n",
    "                            tokens[c_span][type_] = 0\n",
    "                        tokens[c_span][type_] += 1\n",
    "                    elif c_span is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(edit)\n",
    "                        print(\"there's a problem boss\")\n",
    "    \n",
    "    # Remove spans with no annotations from any annotator\n",
    "    if remove_none:\n",
    "        keys = list(tokens.keys())\n",
    "        for entry in keys:\n",
    "            if len(tokens[entry].keys()) == 0:\n",
    "                del tokens[entry]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [x for x in data if x['id'] == 1]\n",
    "get_second_order_annotations_per_token(sents, 'original')\n",
    "\n",
    "# substitution - elaboration, generalization\n",
    "# reorder - word-level, component-level\n",
    "# substitution - paraphrasing\n",
    "\n",
    "# Calculates % Agreement\n",
    "total_agreement = []\n",
    "MAX_HIT = max([x['id'] for x in data])+1\n",
    "for sent_id in range(0, MAX_HIT):\n",
    "    sents = [x for x in data if x['id'] == sent_id]\n",
    "    orig_tokens = get_second_order_annotations_per_token(sents, 'original')\n",
    "    simp_tokens = get_second_order_annotations_per_token(sents, 'simplified')\n",
    "    total_agreement += list(orig_tokens.values()) + list(simp_tokens.values())\n",
    "\n",
    "span_agreement = {}\n",
    "agreement_dims = [\n",
    "    'insertion', \n",
    "    'deletion', \n",
    "    'elaboration_substitution',\n",
    "    'generalization_substitution',\n",
    "    'paraphrasing_substitution',\n",
    "    'word_reorder',\n",
    "    'component_reorder', \n",
    "    'split', \n",
    "    'structure'\n",
    "]\n",
    "\n",
    "for edit_type in agreement_dims:\n",
    "    out = {\n",
    "        3: 0,\n",
    "        2: 0,\n",
    "        1: 0\n",
    "    }\n",
    "    for d in total_agreement:\n",
    "        if edit_type in d.keys():\n",
    "            if d[edit_type] not in out.keys():\n",
    "                out[d[edit_type]] = 0\n",
    "            out[d[edit_type]] += 1\n",
    "    total = sum(out.values())\n",
    "    # Avoid division by 0, doesn't change calculation at all\n",
    "    total = 1 if total == 0 else total\n",
    "    # print(f'{edit_type}: {int(100*out[3]/total)} {int(100*out[2]/total)} {int(100*out[1]/total)}')\n",
    "    span_agreement[edit_type] = {\n",
    "        'three': int(100*out[3]/total),\n",
    "        'two': int(100*out[2]/total)\n",
    "    }\n",
    "span_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "\n",
    "for edit in agreement_dims:\n",
    "    total_agreement = None\n",
    "    MAX_HIT = max([x['id'] for x in data])+1\n",
    "    for sent_id in range(0, MAX_HIT):\n",
    "        sents = [x for x in data if x['id'] == sent_id]\n",
    "        final = []\n",
    "        for sent in sents:\n",
    "            tokens_orig = get_second_order_annotations_per_token([sent], 'original', remove_none=False)\n",
    "            tokens_simp = get_second_order_annotations_per_token([sent], 'simplified', remove_none=False)\n",
    "            all_edits = list(tokens_orig.values()) + list(tokens_simp.values())\n",
    "            formatted = []\n",
    "            for edits in all_edits:\n",
    "                if edit in edits.keys():\n",
    "                    formatted += [1]\n",
    "                else:\n",
    "                    formatted += [0]\n",
    "                # formatted += [nx]\n",
    "            final += [formatted]\n",
    "        \n",
    "        if len(sents) == 3:\n",
    "            if total_agreement is None:\n",
    "                total_agreement = np.asarray(final)\n",
    "            else:\n",
    "                total_agreement = np.append(total_agreement, np.asarray(final), axis=1)\n",
    "    \n",
    "    # Delete columns with all 0s\n",
    "    # idx = np.argwhere(np.all(total_agreement[..., :] == 0, axis=0))\n",
    "    # total_agreement = np.delete(total_agreement, idx, axis=1)\n",
    "\n",
    "    \n",
    "    value_counts = total_agreement\n",
    "    print(f\"{edit} ({total_agreement.shape[1]} tokens)\")\n",
    "    # Having trouble formatting the krippendorff alpha\n",
    "    # print(krippendorff.alpha(value_counts=value_counts, level_of_measurement='nominal'))\n",
    "    print(cohen_kappa_score(total_agreement[0, :], total_agreement[1, :]))\n",
    "    print(cohen_kappa_score(total_agreement[1, :], total_agreement[2, :]))\n",
    "    print(cohen_kappa_score(total_agreement[0, :], total_agreement[2, :]))\n",
    "    agg = irr.aggregate_raters(total_agreement.T)\n",
    "    agg_fleiss = irr.fleiss_kappa(agg[0], method='fleiss')\n",
    "    print(agg_fleiss, end='\\n\\n')\n",
    "\n",
    "    span_agreement[edit]['fleiss'] = agg_fleiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_edit_type_mapping = {\n",
    "    'insertion': '\\\\midrule\\n\\\\ei{Insertion} & Elaboration', \n",
    "    'deletion': '\\\\ed{Deletion} & Generalization', \n",
    "    # 'substitution': '\\\\es{Substitution}', \n",
    "    'elaboration_substitution': '\\\\es{Substitution} & Elaboration',\n",
    "    'generalization_substitution': ' & Generalization',\n",
    "    # 'reorder': '\\\\er{Reorder}', \n",
    "    'word_reorder': '\\\\midrule\\n\\\\er{Reorder} & Word-level',\n",
    "    'component_reorder': ' & Component-level',\n",
    "    'split': '\\\\esp{Split} & Sentence Split', \n",
    "    'structure': '\\\\est{Structure} & Structure',\n",
    "    'paraphrasing_substitution': '\\\\midrule\\n\\\\es{Substitution} & Paraphrase'\n",
    "}\n",
    "table = ''\n",
    "for edit_type in table_edit_type_mapping.keys():\n",
    "    table += f'{table_edit_type_mapping[edit_type]} & {round(span_agreement[edit_type][\"fleiss\"], 2)} & {span_agreement[edit_type][\"three\"]} & {span_agreement[edit_type][\"two\"]} \\\\\\\\\\n'\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_edit_type_mapping = {\n",
    "    'insertion': '\\\\midrule\\n\\\\ei{Insertion} & More Information', \n",
    "    'deletion': '\\\\ed{Deletion} & Less Information', \n",
    "    # 'substitution': '\\\\es{Substitution}', \n",
    "    'elaboration_substitution': '\\\\es{Substitution} & More Information',\n",
    "    'generalization_substitution': ' & Less Information',\n",
    "    # 'reorder': '\\\\er{Reorder}', \n",
    "    'word_reorder': '\\\\midrule\\n\\\\er{Reorder} & Word-level',\n",
    "    'component_reorder': ' & Component-level',\n",
    "    'split': '\\\\esp{Split} & Sentence Split', \n",
    "    'structure': '\\\\est{Structure} & Structure',\n",
    "    'paraphrasing_substitution': '\\\\midrule\\n\\\\es{Substitution} & Same Information'\n",
    "}\n",
    "table = ''\n",
    "for edit_type in table_edit_type_mapping.keys():\n",
    "    table += f'{table_edit_type_mapping[edit_type]} & {round(span_agreement[edit_type][\"fleiss\"], 2)} & {span_agreement[edit_type][\"three\"]} & {span_agreement[edit_type][\"three\"]+span_agreement[edit_type][\"two\"]} \\\\\\\\\\n'\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates agreement for errors\n",
    "error = Error.BAD_DELETION\n",
    "\n",
    "def get_error_stats(error):\n",
    "    total_agreement = None\n",
    "    MAX_HIT = max([x['id'] for x in data])+1\n",
    "    for sent_id in range(0, MAX_HIT):\n",
    "        sents = [x for x in data if x['id'] == sent_id]\n",
    "\n",
    "        sents_errors = []\n",
    "        for sent in sents:\n",
    "            errors = []\n",
    "            for ann in sent['processed_annotations']:\n",
    "                if (error.value == 'grammar_error' and ann['grammar_error']) or ann['error_type'] == error:\n",
    "                    errors += [1]\n",
    "                else:\n",
    "                    errors += [0]\n",
    "            sents_errors += [1 if any(errors) == True else 0]\n",
    "\n",
    "        # Converts error array to 0/1 mapping using alphabetical ordering\n",
    "        if len(sents_errors) == 3:\n",
    "            if total_agreement is None:\n",
    "                total_agreement = np.asarray([sents_errors])\n",
    "            else:\n",
    "                total_agreement = np.append(total_agreement, np.asarray([sents_errors]), axis=0)\n",
    "\n",
    "    if total_agreement.sum() == 0:\n",
    "        print(f'No {error.value} errors', end='\\n\\n')\n",
    "        return None\n",
    "\n",
    "    stats = {}\n",
    "    agg = irr.aggregate_raters(total_agreement)\n",
    "    stats['fleiss'] = irr.fleiss_kappa(agg[0], method='fleiss')\n",
    "\n",
    "    # Delete columns with all 0s\n",
    "    total_agreement = total_agreement.T\n",
    "    idx = np.argwhere(np.all(total_agreement[..., :] == 0, axis=0))\n",
    "    total_agreement = np.delete(total_agreement, idx, axis=1)\n",
    "    total_agreement = total_agreement.T\n",
    "\n",
    "    # % two agree\n",
    "    two_agree = []\n",
    "    for entry in total_agreement:\n",
    "        two_agree.append(np.sum(entry) >= 2)\n",
    "    stats['two'] = 100*sum(two_agree) / total_agreement.shape[0]\n",
    "\n",
    "    # % frequency\n",
    "    stats['freq'] = 100*len(total_agreement) / MAX_HIT\n",
    "\n",
    "    return stats\n",
    "\n",
    "class Tmp(Enum):\n",
    "    GRAMMAR_ERROR = 'grammar_error'\n",
    "\n",
    "table = ''\n",
    "all_stats = {}\n",
    "for error in [e for e in Error] + [Tmp.GRAMMAR_ERROR]:\n",
    "    stats = get_error_stats(error)\n",
    "    if stats is not None:\n",
    "        all_stats[error] = stats\n",
    "\n",
    "sorted_stats = sorted([(k, v['freq']) for k, v in all_stats.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for error in [x[0] for x in sorted_stats]:\n",
    "    table += f'{error.value} & {round(all_stats[error][\"fleiss\"], 2)} & {int(all_stats[error][\"two\"])} & {int(all_stats[error][\"freq\"])} \\\\\\\\ \\n'\n",
    "table = table.replace('grammar_error', 'Grammar Error')\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from scipy.stats import pearsonr\n",
    "import krippendorff\n",
    "\n",
    "# Calculates sentence-score agreement\n",
    "total_agreement = None\n",
    "MAX_HIT = max([x['id'] for x in data])+1\n",
    "for sent_id in range(0, MAX_HIT):\n",
    "    sents = [x for x in data if x['id'] == sent_id]\n",
    "    scores = [sent['score'] for sent in sents]\n",
    "    if len(scores) == 3:\n",
    "        if total_agreement is None:\n",
    "            total_agreement = np.asarray([scores])\n",
    "        else:\n",
    "            total_agreement = np.append(total_agreement, np.asarray([scores]), axis=0)\n",
    "\n",
    "tau, p_value = kendalltau(total_agreement[:, 1], total_agreement[:, 2])\n",
    "r, p_value = pearsonr(total_agreement[:, 1], total_agreement[:, 2])\n",
    "kd = krippendorff.alpha(reliability_data=total_agreement[:300, :].T, level_of_measurement='interval')\n",
    "print(tau)\n",
    "print(r)\n",
    "print(kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the agreeement on all sentences\n",
    "MAX_HIT = 2 # max([x['id'] for x in data])+1\n",
    "for sent_id in range(0, MAX_HIT):\n",
    "    sents = [x for x in data if x['id'] == sent_id]\n",
    "    draw_agreement(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [sent for sent in data \n",
    "    if 'Orion' in sent['original']\n",
    "]\n",
    "for sent_id in set([x['id'] for x in selected]):\n",
    "    sents = [x for x in selected if x['id'] == sent_id]\n",
    "    draw_agreement(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
